from urllib.parse import urljoin, urlparse, urlunparse

import cloudscraper
import sys
import time
from colorama import init, Fore, Style
init()

scraper = cloudscraper.create_scraper()  # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
filter_keywords = ['Gold', '–í–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–æ', '–≥–∞—Ä–∞–Ω—Ç—ñ—è', '–∫–ª—ñ–Ω—ñ—á–Ω—ñ –≤–∏–ø—Ä–æ–±—É–≤–∞–Ω–Ω—è', '–ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞ –Ω–∞—É–∫–æ—é', '–î–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞', 'Palmer\'s', '–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ', '–ù–∞–Ω–µ—Å—ñ—Ç—å', '–ü—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞–Ω–æ', '—É—á–∞—Å–Ω–∏—Ü—å', '–¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è', '–ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç—É–≤–∞—Ç–∏—Å—å', '–ù–∞–Ω–æ—Å—ñ—Ç—å', '–ú—ñ—Å—Ç–∏—Ç—å', '–®—Ä–∞–º–∏', '–†–æ–∑—Ç—è–∂–∫–∏', '–ë–µ–∑ –º—ñ–Ω–µ—Ä–∞–ª—å–Ω–æ—ó –æ–ª—ñ—ó', '–ë–µ–∑ –ø–∞—Ä–∞–±–µ–Ω—ñ–≤', '–ë–µ–∑ —Ñ—Ç–∞–ª–∞—Ç—ñ–≤', '–®–≤–∏–¥–∫–µ –ø–æ–≥–ª–∏–Ω–∞–Ω–Ω—è', '–ì–ª–∏–±–æ–∫–µ –∑–≤–æ–ª–æ–∂–µ–Ω–Ω—è', '–§–ª–∞–∫–æ–Ω —ñ–∑ –¥–æ–∑–∞—Ç–æ—Ä–æ–º', '–ù–µ–∫–æ–º–µ–¥–æ–≥–µ–Ω–Ω–∏–π –∑–∞—Å—ñ–±' '—Å—ñ–º–µ–π–Ω–æ–º—É –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤—ñ', '–í–∏—Ä–æ–±–ª–µ–Ω–æ', '–≤–∏—Ä–æ–±–ª–µ–Ω–æ', '–ö–æ–º–ø–∞–Ω—ñ—é –∑–∞—Å–Ω–æ–≤–∞–Ω–æ', '–ü—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç—É–π—Ç–µ—Å—è –∑ –ª—ñ–∫–∞—Ä–µ–º', '–ù–µ —Å–ª—ñ–¥ –∫—É–ø—É–≤–∞—Ç–∏ –ø—Ä–æ–¥—É–∫—Ç, —è–∫—â–æ –∑–æ–≤–Ω—ñ—à–Ω—è –∑–∞—Ö–∏—Å–Ω–∞ –ø–ª—ñ–≤–∫–∞ –ø–æ—à–∫–æ–¥–∂–µ–Ω–∞', '–ù–µ —Å–ª—ñ–¥ –ø–µ—Ä–µ–≤–∏—â—É–≤–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—É –¥–æ–∑—É.', '–ó–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ–º—É –¥–ª—è –¥—ñ—Ç–µ–π –º—ñ—Å—Ü—ñ.', '–°—ñ–º–µ–π–Ω–µ –ø—ñ–¥–ø—Ä–∏—î–º—Å—Ç–≤–æ', '–ë–µ–∑ –ì–ú–û: —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç Non GMO LE Certified', '–°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç', 'Certified', '–ü–µ—Ä–µ–¥ –ø—Ä–∏–π–æ–º–æ–º', '–ù–µ –∫—É–ø—É–π—Ç–µ, —è–∫—â–æ –∑–æ–≤–Ω—ñ—à–Ω—è –∑–∞—Ö–∏—Å–Ω–∞ –ø–ª—ñ–≤–∫–∞ –ø–æ—à–∫–æ–¥–∂–µ–Ω–∞ –∞–±–æ –ø–æ—à–∫–æ–¥–∂–µ–Ω–∞', '–í—ñ–∑—å–º—ñ—Ç—å –æ–¥–Ω—É –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –≤–∏–º—ñ—Ä—è–Ω—É –¥–æ–∑—É.', '–†–æ–∑–∫—Ä–∏–π—Ç–µ –ø–∞–∫–µ—Ç —ñ–∑ —Ñ–æ–ª—å–≥–∏ –Ω–∞ –Ω–∞–¥—Ä—ñ–∑–∞–Ω–æ–º—É –∫—ñ–Ω—Ü—ñ', '–°–≤—ñ—Ç–æ–≤–∏–π –ª—ñ–¥–µ—Ä —É –≥–∞–ª—É–∑—ñ –≥–æ–º–µ–æ–ø–∞—Ç–∏—á–Ω–∏—Ö –ø—Ä–µ–ø–∞—Ä–∞—Ç—ñ–≤', '–ó–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤ –ø—Ä–æ—Ö–æ–ª–æ–¥–Ω–æ–º—É –º—ñ—Å—Ü—ñ, –∑–∞—Ö–∏—â–µ–Ω–æ–º—É –≤—ñ–¥ –ø—Ä—è–º–∏—Ö —Å–æ–Ω—è—á–Ω–∏—Ö –ø—Ä–æ–º–µ–Ω—ñ–≤.', '–ü—Ä–∏–ø–∏–Ω—ñ—Ç—å –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ç–∞ –∑–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ –ª—ñ–∫–∞—Ä—è, —è–∫—â–æ –∑‚Äô—è–≤–ª—è—î—Ç—å—Å—è –≤–∏—Å–∏–ø.', '–ó–±–µ—Ä—ñ–≥–∞–π—Ç–µ –Ω–µ–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –ø–∞—Ç—á—ñ –≤ –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ñ–π —É–ø–∞–∫–æ–≤—Ü—ñ —ñ —Ç—Ä–∏–º–∞–π—Ç–µ —ó—ó —â—ñ–ª—å–Ω–æ –∑–∞–∫—Ä–∏—Ç–æ—é –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–≤—ñ–∂–æ—Å—Ç—ñ', '–ù–∞–Ω–æ—Å—å—Ç–µ –Ω–∞ —á–∏—Å—Ç—É —Å—É—Ö—É —à–∫—ñ—Ä—É –¥–æ –±—É–¥—å-—è–∫–∏—Ö —ñ–Ω—à–∏—Ö –∫—Ä–æ–∫—ñ–≤ –∑ –¥–æ–≥–ª—è–¥—É –∑–∞ —à–∫—ñ—Ä–æ—é', '–ü–µ—Ä–µ–¥ –ø—Ä–∏–π–º–∞–Ω–Ω—è–º –¥—ñ—î—Ç–∏—á–Ω–∏—Ö –¥–æ–±–∞–≤–æ–∫ –ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç—É–π—Ç–µ—Å—è –∑ –ª—ñ–∫–∞—Ä–µ–º, —è–∫—â–æ –≤–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç–µ –∫—É—Ä—Å –ª—ñ–∫—É–≤–∞–Ω–Ω—è –≤—ñ–¥ –∑–∞—Ö–≤–æ—Ä—é–≤–∞–Ω–Ω—è, –∞ —Ç–∞–∫–æ–∂ —É –ø–µ—Ä—ñ–æ–¥ –≤–∞–≥—ñ—Ç–Ω–æ—Å—Ç—ñ —Ç–∞ –≥—Ä—É–¥–Ω–æ–≥–æ –≤–∏–≥–æ–¥—É–≤–∞–Ω–Ω—è.', '–°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç Non-GMO Project Verified', '–í–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –∑ –Ω–∞–π–∫—Ä–∞—â–∏—Ö –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–∏—Ö –ø–ª–æ–¥—ñ–≤ –º–æ–Ω–∞—Ö–∞', '1:1, —è–∫ —Ü—É–∫–æ—Ä', '–ü–æ–≤–Ω—ñ—Å—Ç—é –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç', '–ù–∞—Ç—É—Ä–∞–ª—å–Ω—ñ –ø—Ä–æ–¥—É–∫—Ç–∏ –±–µ–∑ –≥–ª—é—Ç–µ–Ω—É', '–ü—Ä–æ—Å—Ç–∏–π —ñ —á–∏—Å—Ç–∏–π —Å–ø–∏—Å–æ–∫ —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤', '–í–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –∑—ñ —Å–ø—Ä–∞–≤–∂–Ω—ñ—Ö —Ñ—Ä—É–∫—Ç—ñ–≤ —ñ –æ–≤–æ—á—ñ–≤', '–ë–µ–∑ —à—Ç—É—á–Ω–∏—Ö —ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç—ñ–≤', '–°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç USDA Organic', '–°–µ—Ä—Ç–∏—Ñ—ñ–∫–∞—Ç Non-GMO\xa0Project\xa0Verified', '–û—Ä–≥–∞–Ω—ñ—á–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç, —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏–π QAI',  '–î–∞—Ç–∞', '–ö–æ–∫–æ—Å', '–û–≤–µ—Å', '–ì—Ä—É—à–∞', '–Ø–±–ª—É–∫–æ', '–ü–æ–ª—É–Ω–∏—Ü—è', '–ú–∞–ª–∏–Ω–∞', '–ë—É—Ä—è–∫', '–ì–∞—Ä–±—É–∑', '–í–∞–Ω—ñ–ª—å', 'Cosmos Natural —Å–µ—Ä—Ç–∏—Ñ—ñ–∫–æ–≤–∞–Ω–æ Ecocert Greenlife –∑–≥—ñ–¥–Ω–æ –∑—ñ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º COSMOS', '—à—Ç—É—á–Ω–∏—Ö –∞—Ä–æ–º–∞—Ç–∏–∑–∞—Ç–æ—Ä—ñ–≤;', '—Ñ—Ç–∞–ª–∞—Ç—ñ–≤;', '—Å—É–ª—å—Ñ–∞—Ç—ñ–≤;', '–ø–∞—Ä–∞–±–µ–Ω—ñ–≤;', '–ï–î–¢–ö;', '–≥–ª—é—Ç–µ–Ω—ñ–≤;', '–±–∞—Ä–≤–Ω–∏–∫—ñ–≤.', '–°–∫—Ä—É—Ç—ñ—Ç—å', '–í–∏–¥–∞–≤—ñ—Ç—å', '–î—ñ—î—Ç–∏—á–Ω–∞ –¥–æ–±–∞–≤–∫–∞', '–ú–æ–∂–Ω–∞ –¥–∞–≤–∞—Ç–∏ –æ–∫—Ä–µ–º–æ –∞–±–æ –ª–µ–≥–∫–æ –¥–æ–¥–∞–≤–∞—Ç–∏ –¥–æ —Å—É–º—ñ—à—ñ, –º–æ–ª–æ–∫–∞, —Å–æ–∫—É –∞–±–æ —ó–∂—ñ', '–ü—Ä–æ–≤—ñ–¥–Ω–∏–π –≤–∏—Ä–æ–±–Ω–∏–∫ –º–µ–ª–∞—Ç–æ–Ω—ñ–Ω—É', '–ù–µ –º—ñ—Å—Ç–∏—Ç—å —Ä–µ—á–æ–≤–∏–Ω, —â–æ –≤–µ–¥—É—Ç—å –¥–æ –∑–≤–∏–∫–∞–Ω–Ω—è', '–î–ª—è –¥—ñ—Ç–µ–π –≤—ñ–¥ 4 —Ä–æ–∫—ñ–≤', '–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –ø–µ–¥—ñ–∞—Ç—Ä–∞–º–∏', '–ù–∞–π—Å–æ–ª–æ–¥—à–∏–π —Å–æ–Ω']


import re
import math
from bs4 import BeautifulSoup

def parse_summary(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Cookie': 'iher-pref1=storeid=0&sccode=UA&lan=uk-UA&scurcode=UAH&wp=2; '
                  'ih-preference=store=0; '
                  'ihr-lac=rturl%3Dhttp%3A%2F%2Fcatalog.app.iherb.com%2Fcatalog%2FcurrentUser'
    }

    response = scraper.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else '–ë–µ–∑ –Ω–∞–∑–≤–∏'
    print(f"[DEBUG] –ù–∞–∑–≤–∞–Ω–∏–µ: {title}")

    # –û–ø–∏—Å–∞–Ω–∏–µ
    desc_div = soup.find('div', class_='prodOverviewDetail')
    description = desc_div.get_text(strip=True) if desc_div else '–û–ø–∏—Å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π'
    print(f"[DEBUG] –û–ø–∏—Å–∞–Ω–∏–µ: {description}")

    # –¶–µ–Ω–∞
    price_div = soup.find('div', class_='list-price')
    price = price_div.get_text(strip=True) if price_div else '–¶—ñ–Ω–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞'
    print(f"[DEBUG] –¶–µ–Ω–∞ (—Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç): {price}")

    price_numeric = None
    discount_percent = 20

    if price != '–¶—ñ–Ω–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞':
        price_clean = re.sub(r'[^\d\.]', '', price.replace(',', '.'))
        print(f"[DEBUG] –û—á–∏—â–µ–Ω–Ω–∞—è —Ü–µ–Ω–∞: {price_clean}")
        try:
            price_numeric = float(price_clean)
            print(f"[DEBUG] –ß–∏—Å–ª–æ–≤–∞—è —Ü–µ–Ω–∞: {price_numeric}")
        except ValueError:
            price_numeric = None
            print(f"[DEBUG] –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ü–µ–Ω—ã")

        # –°–∫–∏–¥–∫–∞
        discount_div = soup.find('div', class_='discount-title')
        if discount_div:
            match = re.search(r'(\d{1,2})\s*%', discount_div.get_text())
            if match:
                if discount_percent < int(match.group(1)):
                    discount_percent = int(match.group(1))
                    print(f"[DEBUG] –°–∫–∏–¥–∫–∞ –Ω–∞–π–¥–µ–Ω–∞: {discount_percent}%")
            else:
                print("[DEBUG] –°–∫–∏–¥–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–æ div –µ—Å—Ç—å")


        print(f"[DEBUG] –°–∫–∏–¥–∫–∞: {discount_percent}")

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
        if price_numeric is not None:
            discount_multiplier = 1 - discount_percent / 100
            price_numeric *= discount_multiplier
            price_numeric *= 1.05  # –ù–î–°
            price_numeric = math.ceil(price_numeric)
            print(f"[DEBUG] –§–∏–Ω–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ —Å —É—á–µ—Ç–æ–º —Å–∫–∏–¥–∫–∏ –∏ –ù–î–°: {price_numeric}")
    else:
        print("[DEBUG] –¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    col_divs = soup.find_all('div', class_='col-xs-24')
    li_items = [li.get_text(strip=True) for col in col_divs for li in col.find_all('li')]
    li_half = li_items[:len(li_items) // 2]
    print(f"[DEBUG] –ü–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {li_half[:6]}")

    return title, description, price, price_numeric, li_half, url, discount_percent


def get_links(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Cookie': 'iher-pref1=storeid=0&sccode=UA&lan=uk-UA&scurcode=UAH&wp=2; '
                  'ih-preference=store=0; '
                  'ihr-lac=rturl%3Dhttp%3A%2F%2Fcatalog.app.iherb.com%2Fcatalog%2FcurrentUser'
    }

    response = scraper.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    div = soup.find('div', class_='products product-cells clearfix')

    def normalize_link(link):
        link = urljoin("https://ua.iherb.com", link)
        parsed = urlparse(link)
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))

    links = [normalize_link(a['href']) for a in div.find_all('a', href=True)]

    filtered_links = [
        link for link in links
        if not re.match(r'https://ua\.iherb\.com/r', link)
        and not re.match(r'.*/New-Products', link)
        and not re.match(r'.*/Specials', link)
        and not re.match(r'.*/Trial-Pricing', link)
        and not link.startswith('#')
        and not re.search(r'21st-century', link)
        and not re.search(r'nmn', link)
    ]

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    filtered_links = list(dict.fromkeys(filtered_links))

    val = int(sys.argv[1])
    return filtered_links[:val]


def get_image(url, file_name, max_retries=5, delay=3):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36',
        'Cookie': 'iher-pref1=storeid=0&sccode=UA&lan=uk-UA&scurcode=UAH&wp=2&ifv=1&accsave=0&lchg=1;'
    }

    attempt = 0
    while attempt < max_retries:
        try:
            response = scraper.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                break
            else:
                print(f"‚ö†Ô∏è –°–ø—Ä–æ–±–∞ {attempt + 1}: –°—Ç–∞—Ç—É—Å {response.status_code}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")
        except Exception as e:
            print(f"‚ö†Ô∏è –°–ø—Ä–æ–±–∞ {attempt + 1}: –ü–æ–º–∏–ª–∫–∞ ‚Äî {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay} —Å–µ–∫...")

        attempt += 1
        time.sleep(delay)
    else:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É –ø—ñ—Å–ª—è –∫—ñ–ª—å–∫–æ—Ö —Å–ø—Ä–æ–±.")
        return

    soup = BeautifulSoup(response.text, 'html.parser')
    img_tag = soup.find('img', id='iherb-product-image')

    if img_tag and img_tag.get('src'):
        img_url = img_tag['src']
        print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑: {img_url}")
        img_data = scraper.get(img_url, headers=headers).content
        with open(file_name, 'wb') as f:
            f.write(img_data)
        print(f"‚úÖ –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª {file_name}")
    else:
        print("‚ùå –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")


def parse_summary(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36',
        'Accept-Language': 'uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7',
        'Cookie': 'iher-pref1=storeid=0&sccode=UA&lan=uk-UA&scurcode=UAH&wp=2; '
                  'ih-preference=store=0; '
                  'ihr-lac=rturl%3Dhttp%3A%2F%2Fcatalog.app.iherb.com%2Fcatalog%2FcurrentUser'
    }

    MAX_CHARS = 850

    response = scraper.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else '–ë–µ–∑ –Ω–∞–∑–≤–∏'

    # –û–ø–∏—Å–∞–Ω–∏–µ
    desc_div = soup.find('div', class_='prodOverviewDetail')
    description = desc_div.get_text(strip=True) if desc_div else '–û–ø–∏—Å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π'

    # –¶–µ–Ω–∞
    price_div = soup.find('div', class_='list-price')
    price = price_div.get_text(strip=True) if price_div else '–¶—ñ–Ω–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞'

    price_numeric = None
    discount_percent = 20

    if price != '–¶—ñ–Ω–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞':
        price_clean = re.sub(r'[^\d\.]', '', price.replace(',', '.'))
        try:
            price_numeric = float(price_clean)
        except ValueError:
            price_numeric = None

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–∏–¥–∫–∏
        discount_div = soup.find('div', class_='discount-title')
        if discount_div:
            match = re.search(r'(\d{1,2})\s*%', discount_div.get_text())
            if match:
                discount_percent = max(discount_percent, int(match.group(1)))

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
        if price_numeric is not None:
            price_numeric *= (1 - discount_percent / 100)
            price_numeric *= 1.05  # –ù–î–°
            price_numeric = math.ceil(price_numeric)

    # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    col_divs = soup.find_all('div', class_='col-xs-24')
    li_items = [li.get_text(strip=True) for col in col_divs for li in col.find_all('li')]
    li_half = li_items[:len(li_items) // 2]

    def build_text(features_list):
        parts = [
            f"–ù–∞–∑–≤–∞: {title}",
            f"–û–ø–∏—Å: {description}",
            f"–¶—ñ–Ω–∞: {price_numeric if price_numeric else price}",
            f"–ó–Ω–∏–∂–∫–∞: {discount_percent}%",
            f"–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {', '.join(features_list)}" if features_list else "",
            f"–ü–æ—Å–∏–ª–∞–Ω–Ω—è: {url}"
        ]
        return "\n".join(part for part in parts if part)

    summary_text = build_text(li_half)

    # üîÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
    while len(summary_text) > MAX_CHARS and li_half:
        removed = li_half.pop()
        print(f"[DEBUG] –í–∏–¥–∞–ª–µ–Ω–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É: {removed}")
        summary_text = build_text(li_half)

    # üî™ –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë –¥–ª–∏–Ω–Ω–µ–µ ‚Äî —Å–æ–∫—Ä–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
    if len(summary_text) > MAX_CHARS:
        excess = len(summary_text) - MAX_CHARS
        original_len = len(description)
        description = description[:-excess - 5].rsplit(' ', 1)[0] + "..."
        print(f"[DEBUG] –°–∫–æ—Ä–æ—á–µ–Ω–æ –æ–ø–∏—Å –∑ {original_len} –¥–æ {len(description)} —Å–∏–º–≤–æ–ª—ñ–≤")
        summary_text = build_text(li_half)

    print(f"[DEBUG] –ò—Ç–æ–≥–æ–≤–∞—è –¥–ª–∏–Ω–∞: {len(summary_text)} —Å–∏–º–≤–æ–ª—ñ–≤, –∑–∞–ª–∏—à–∏–ª–æ—Å—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: {len(li_half)}")

    return title, description, price, price_numeric, li_half, url, discount_percent


def print_text(url, file):
    title, description, price, price_numeric, items, link, discount_percent = parse_summary(url)

    # üß© –§–∏–ª—å—Ç—Ä-—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å
    filter_keywords = ["–º—ñ—Å—Ç–∏—Ç—å", "—ñ–Ω–≥—Ä–µ–¥—ñ—î–Ω—Ç–∏", "—Å–∫–ª–∞–¥", "–∑–±–µ—Ä—ñ–≥–∞—Ç–∏"]

    with open(file, "w", encoding="utf-8") as f:
        f.write(f"üî• -{discount_percent}% –¶i–Ω–∞ {price_numeric} –≥—Ä–Ω. *{title}*\n\n")
        for item in items:
            if all(keyword.lower() not in item.lower() for keyword in filter_keywords):
                f.write(f"‚úÖ {item.replace('*', '')}\n")
        f.write(f"\n‚úèÔ∏è *–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –ø–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—é*\n{description}\n")
        f.write(f"\nüîó {link}\n")

    print(link)
    print(f"{Fore.MAGENTA}{file}{Style.RESET_ALL} - {Fore.GREEN}success!{Style.RESET_ALL}")

def main():
    if len(sys.argv) != 3:
        print("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: python parse_summary.py <LNKS_NUM> <URL>")
        sys.exit(1)

    url = sys.argv[2]

    while True:
        try:
            links = get_links(url)
            break
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω—å: {e}. –ü–æ–≤—Ç–æ—Ä —Å–ø—Ä–æ–±–∏...")

    print(links)

    for i in range(len(links)):
        while True:
            try:
                print_text(links[i], f"C:/Users/sylv/Documents/iherb_parser_data/{i}.txt")
                get_image(links[i], f"C:/Users/sylv/Documents/iherb_parser_data/{i}.jpg")
                i += 1
                break
            except Exception as e:
                print(f"{Fore.RED}–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ —Ç–µ–∫—Å—Ç—É: {e}. –ü–æ–≤—Ç–æ—Ä —Å–ø—Ä–æ–±–∏...{Style.RESET_ALL}")



if __name__ == "__main__":
    main()
